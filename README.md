# Нейросетевые алгоритмы аудиовизуального распознавания эмоций
**Работа выполнена бакалавром по направлению "Прикладная информатика и математика" Сидоровой Анной Павловной в рамках Выпускной Квалификационной Работы (ВКР) и 6th Workshop and Competition on Affective Behavior Analysis in-the-wild (ABAW) при IEEE Computer Vision and Pattern Recognition Conference (CVPR), 2024.**

Данный репозиторий предоставляет общие пайплайны для воспроизведения экспериментов, представленных в работах [Neural Network Algorithms for Audiovisual Emotion Recognition](https://disk.yandex.ru/i/hbC4upbdPw5DXg) и [EmotiEffNet and Temporal Convolutional Networks in Video-based Facial Expression Recognition and Action Unit Detection](https://disk.yandex.ru/i/q3EQEeLX7Htp9Q). Код без изменений доступен по [ссылке](https://disk.yandex.ru/d/1pS_leUrBP6m_A), ноутбуки с названием экспериментов доступны по [ссылке](https://disk.yandex.ru/d/Qv6MMa2Sm2EjsA), код с экспериментами для ABAW6 также доступен по [ссылке](https://github.com/av-savchenko/face-emotion-recognition/blob/main/src/ABAW/ABAW6/abaw6_affwild2.ipynb).

Все эксперименты логгировались с помощью CometML, проект находится в [свободном доступе](https://www.comet.com/annanet/abaw6/view/new/panels).

Работа посвящена аффективным вычислениям, которые разрабатывают модели и методы для понимания человеческих эмоции. Основными целями данного исследовательского проекта являются: изучение соответствующих алгоритмов и создание на их основе модели распознавания эмоциональных состояний. В результате мы заняли третье место на Action Unit Detection и четвертое место на Expression Recognition Challenges. Наши результаты заметно превзошли бейзлайн. Macro-averaged F1-score для EXPR превзошел базовую модель на ~52%, а AU примерно на 34%.

Предлагаемый подход:
![Предлагаемая модель EmotiEffNet + TCN](https://github.com/Laitielly/nn_emorec/blob/main/img/TCN_model.png)

В целях исследования и участия в воркшопе были проделаны следующие шаги:
- Исследование литературы: изучение актуальных методов работы с AV данными (цифровая обработка изображений и обработка аудио), state-of-the-art пайплайны для решения поставленных проблем. Ноутбуки для выделения акустических признаков ([OpenL3](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/openl3.ipynb), [wav2vec2-hubert](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/wav2vec-3.ipynb), [wav2vec2-large-robust-emotion](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/wav2vec2-large.ipynb)), визуальных признаков ([EmotiEffNet](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/get_EmotiEffNet_features.ipynb)), объеденение данных ([EmotiEffNet](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/complete_video%2Baudio_fea.ipynb), [MAE+EmotiEffNet](https://github.com/Laitielly/nn_emorec/blob/main/feature%20extraction/complete_video%2Baudio_fea-mae.ipynb)) и работа с датасетами ([crop & align](https://github.com/Laitielly/nn_emorec/blob/main/MAE-encoder/notebooks/Crop_and_align_images_pipeline.ipynb) - RetinaFace).
- Реализация, опирающаяся на изученные методы. [Пайплайны c Temporal Convolutional Networks](https://github.com/Laitielly/nn_emorec/tree/main/TCN%20pipelines), [обучение MAE](https://github.com/Laitielly/nn_emorec/blob/main/MAE-encoder/notebooks/MAE-train.ipynb), дообучение MAE [энкодер](https://github.com/Laitielly/nn_emorec/blob/main/MAE-encoder/notebooks/MAE_finetune_training_encoder.ipynb)/[енкодер+декодер](https://github.com/Laitielly/nn_emorec/blob/main/MAE-encoder/notebooks/MAE_finetune_training-enc_dec.ipynb), [внедрение MAE в общий пайплайн](https://github.com/Laitielly/nn_emorec/tree/main/MAE-encoder/tcn%2Bmae). Реализация архитектур [TCN](https://github.com/Laitielly/nn_emorec/tree/main/TCN%20pipelines/tcn), [MAE](https://github.com/Laitielly/nn_emorec/tree/main/MAE-encoder/mae). [Примеры финального предсказания](https://github.com/Laitielly/nn_emorec/tree/main/TCN%20pipelines/prediction%20files).
- Документирование методов.
