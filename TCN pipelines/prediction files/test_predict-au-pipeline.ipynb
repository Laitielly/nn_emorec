{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submit for AU\n",
    "\n",
    "## imports & load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader,Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'fea_notebooks/features_newvf2.pickle'\n",
    "test_dir = '/home/HDD6TB/datasets/emotions/ABAW/ABAW_5/VA_AU_FER/test_set/CVPR_5th_ABAW_AU_test_set_sample.txt'\n",
    "cropped_data = '/home/avsavchenko/src/emotions-multimodal/faces/ABAW/abaw5/enet_b0_8_best_vgaf_cropped.pickle'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2941546\n"
     ]
    }
   ],
   "source": [
    "with open(data_dir, 'rb') as handle:\n",
    "    data=pickle.load(handle)\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.0.1+cu118\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "print(f\"Torch: {torch.__version__}\")\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model & dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tcn import TemporalConvNet\n",
    "from trans_encoder import TransEncoder\n",
    "\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, modality=['frames', 'w2v2large', 'openl3', 'w2v2hub'],\n",
    "                 embedding_dim={'frames': 1280, 'w2v2large': 1024, 'openl3': 512,\n",
    "                                'w2v2hub': 256},\n",
    "                 tcn_channel={\n",
    "                     'frames': [1280, 512, 256, 128],\n",
    "                     'w2v2large': [1024, 512, 256, 128],\n",
    "                     'openl3': [512, 256, 128],\n",
    "                     'w2v2hub': [256, 128]\n",
    "    }):\n",
    "        super(Model, self).__init__()\n",
    "        self.modality = modality\n",
    "\n",
    "        self.temporal, self.fusion = nn.ModuleDict(), None\n",
    "\n",
    "        for modal in self.modality:\n",
    "            self.temporal[modal] = TemporalConvNet(num_inputs=embedding_dim[modal],\n",
    "                                                   num_channels=tcn_channel[modal], dropout=0.3, attention=False)\n",
    "\n",
    "        conv_dim = 0\n",
    "        for m in self.modality:\n",
    "            conv_dim += tcn_channel[m][-1]\n",
    "            \n",
    "        self.encoder = TransEncoder(\n",
    "            inc=conv_dim, outc=256, dropout=0.3, nheads=4, \n",
    "            nlayer=8)\n",
    "            \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 256//2),\n",
    "            nn.BatchNorm1d(256//2),\n",
    "            nn.Linear(256//2, 12),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        bs, seq_len, _ = x[self.modality[0]].shape\n",
    "#         print(bs, seq_len)\n",
    "        for m in self.modality:\n",
    "            x[m] = x[m].transpose(1, 2)\n",
    "            x[m] = self.temporal[m](x[m])\n",
    "\n",
    "        feat_list = []\n",
    "        for m in self.modality:\n",
    "            feat_list.append(x[m])\n",
    "        out = torch.cat(feat_list, dim=1)\n",
    "        out = self.encoder(out)\n",
    "\n",
    "        out = torch.transpose(out, 1, 0)\n",
    "        out = torch.reshape(out, (bs*seq_len, -1))\n",
    "#         print(out.shape)\n",
    "\n",
    "        out = self.head(out)\n",
    "        return F.sigmoid(out)\n",
    "\n",
    "model = Model(modality=['frames'], embedding_dim={'frames': 1280},\n",
    "              tcn_channel={\n",
    "                     'frames': [1280, 512, 256, 128]})\n",
    "model.to(device);\n",
    "model.load_state_dict(torch.load('fau_ovid_52.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import ceil\n",
    "\n",
    "\n",
    "class audioDataset(Dataset):\n",
    "    def __init__(self, names, values, window=300, step=200):\n",
    "        self.data = values\n",
    "        self.lenghts_of_seq = len(self.data[0])\n",
    "#         self.y = y\n",
    "        \n",
    "        self.names = names\n",
    "        self.window = window\n",
    "        self.step = step\n",
    "        \n",
    "        self.len = ceil((self.lenghts_of_seq - self.window) / self.step) + 1\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if idx == self.len-1:\n",
    "            stride = [torch.tensor(i[-self.window::]) for i in self.data]\n",
    "#             y = self.y[-self.window::]\n",
    "        \n",
    "        else:\n",
    "            stride = [torch.tensor(i[idx*self.step:idx*self.step + self.window]) for i in self.data]\n",
    "#             y = self.y[idx*self.step:idx*self.step + self.window]\n",
    "            \n",
    "        r_dict = dict([(i, j.to(device)) for i, j in zip(self.names, stride)])\n",
    "        \n",
    "        return r_dict, len(self.data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/HDD6TB/datasets/emotions/ABAW/ABAW_6/6th_ABAW_Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data, model):\n",
    "    pred_labels_val = []\n",
    "\n",
    "    a_dataset = audioDataset(['frames'],\n",
    "                     data, window=300, step=300)\n",
    "    training_loader = DataLoader(a_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(training_loader):\n",
    "            vinputs, d_len = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            pred_labels_val += voutputs.tolist()\n",
    "    \n",
    "    d_len = d_len[0]\n",
    "\n",
    "    s = 300 - len(pred_labels_val) + d_len\n",
    "    return pred_labels_val[:-300]+pred_labels_val[-s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/105 [00:00<?, ?it/s]<ipython-input-6-786bd96cdc3b>:25: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:245.)\n",
      "  stride = [torch.tensor(i[idx*self.step:idx*self.step + self.window]) for i in self.data]\n",
      "100%|██████████| 105/105 [00:59<00:00,  1.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "data_dir=os.path.join(DATA_DIR,'faces')\n",
    "dirpath=os.path.join(DATA_DIR,'AU_Detection_Challenge/Validation_Set')\n",
    "test_videos={}\n",
    "for filename in tqdm(os.listdir(dirpath)):\n",
    "    fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "    if ext.lower()=='.txt':\n",
    "        X,indices,expressions=[],[],[]\n",
    "        w2v2large_t, openl3_t, w2v2hub_t = [], [], []\n",
    "        with open(os.path.join(dirpath,filename)) as f:\n",
    "            lines = f.read().splitlines()\n",
    "            prev_val=None\n",
    "            for i,line in enumerate(lines):\n",
    "                if i>0:\n",
    "                    splitted_line=line.split(',')\n",
    "                    aus=list(map(int,splitted_line))\n",
    "                    if min(aus)>=0:\n",
    "                        imagename=fn+'/'+str(i).zfill(5)+'.jpg'\n",
    "                        if imagename in data:\n",
    "                            X.append(data[imagename]['frame'][0])\n",
    "#                             w2v2large_t.append(data[imagename]['w2v2large']) \n",
    "#                             openl3_t.append(data[imagename]['openl3']) \n",
    "#                             w2v2hub_t.append(data[imagename]['w2v2hub'])\n",
    "\n",
    "                            indices.append(i)\n",
    "                            expressions.append(aus)\n",
    "                        \n",
    "        test_videos[fn]=(predict([X], model),indices,np.array(expressions))\n",
    "print(len(test_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "thresh = np.array([0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 12/12 [07:24<00:00, 37.07s/it]\n"
     ]
    }
   ],
   "source": [
    "for i in tqdm(range(12)):\n",
    "    best_j, best_f1 = 0, 0\n",
    "    for j in np.linspace(0,1,11):\n",
    "        thresh[i] = j\n",
    "        predicts = []\n",
    "        y_true = []\n",
    "        for filename in os.listdir(dirpath):\n",
    "            fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "            predicts+=(np.array(test_videos[fn][0]) >= thresh[None,:]).tolist()\n",
    "            y_true += test_videos[fn][2].tolist()\n",
    "            \n",
    "        f1 = f1_score(y_true,\n",
    "                        predicts,\n",
    "                        average='macro')\n",
    "        \n",
    "        if f1 > best_f1:\n",
    "            best_f1 = f1\n",
    "            best_j = j\n",
    "            \n",
    "    thresh[i] = best_j "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.4, 0.2, 0.2, 0.2, 0.4, 0.3, 0.2, 0.2, 0.2, 0.3, 0.4, 0.2])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "thresh[thresh <= 0.1] = 0.2\n",
    "thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5495948264673347"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicts = []\n",
    "y_true = []\n",
    "for filename in os.listdir(dirpath):\n",
    "    fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "    predicts+=(np.array(test_videos[fn][0]) >= thresh[None,:]).tolist()\n",
    "    y_true += test_videos[fn][2].tolist()\n",
    "    \n",
    "f1_score(y_true,\n",
    "         predicts,\n",
    "         average='macro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing on validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.5495948264673347 [0.581114237110704, 0.4666320863327556, 0.6041993147228347, 0.6352552654421376, 0.7495249497490196, 0.7534327962096743, 0.7448095188107877, 0.30390911129381865, 0.21175821577744502, 0.2488874054294615, 0.8423092968775072, 0.45330571985187007]\n",
      "3 0.5506883034396232 [0.582698033570003, 0.46840849139555335, 0.6043185365179083, 0.6373722511631607, 0.7497643164079627, 0.7534411035542563, 0.7460587655867706, 0.3091184400044318, 0.20933788074797127, 0.2503534107603182, 0.8416181457546869, 0.4557702658124555]\n",
      "5 0.5504932707588317 [0.5828912412637429, 0.46990330076709724, 0.6043566329507241, 0.6384761451469765, 0.7493180182719116, 0.7522121246567391, 0.7464266185847466, 0.30894428152492664, 0.20498593310409502, 0.2517351030001383, 0.8404647145505083, 0.45620513528437323]\n",
      "7 0.550203256585771 [0.5833292497672368, 0.4714679651206121, 0.6044538429406852, 0.639046700783703, 0.7490246434621058, 0.7510542734609397, 0.7461824073448998, 0.3103122730573711, 0.20026597825236644, 0.2524937139067724, 0.8390685516647806, 0.45573947926777936]\n",
      "10 0.5495414249072537 [0.5828530376998057, 0.4725003699501801, 0.6040833809251857, 0.640667645406982, 0.7477898180331222, 0.7486677716587357, 0.7451169692581369, 0.3095572101722959, 0.19561975037287072, 0.2553520194217612, 0.8372030860435922, 0.45508603994437474]\n",
      "15 0.5474948110377696 [0.5829150658056078, 0.46893000097627646, 0.6048803710780436, 0.6426291153371589, 0.7451255298337138, 0.7454182008226128, 0.7430920281230229, 0.30562810834019105, 0.18004943784387212, 0.26097970647355234, 0.8351944673561322, 0.45509570046305314]\n",
      "20 0.5450014899350453 [0.5810374498074244, 0.4685574263671211, 0.6031395430490211, 0.6442684095097767, 0.7422369938275791, 0.7428574787781881, 0.7406418755762392, 0.3017142857142857, 0.1640884425366486, 0.26330239964856406, 0.833403214756425, 0.45477035964927043]\n",
      "25 0.5434062358334671 [0.5800757656687842, 0.46803380471867584, 0.6023309491198701, 0.6459443657922583, 0.7401476264644139, 0.7401731690522584, 0.7382867300436552, 0.29632807751439094, 0.15619499815807783, 0.2650246305418719, 0.8320944489451717, 0.4562402639821778]\n",
      "50 0.5365974466995198 [0.569338539659131, 0.46845466757795007, 0.5957704524948398, 0.6493808958111275, 0.7321537457323446, 0.7310406968013214, 0.7272711936603531, 0.28564003564003565, 0.1299193512846503, 0.26772718664610556, 0.8276164961131364, 0.45485609897324236]\n"
     ]
    }
   ],
   "source": [
    "deltas=[0,3,5,7,10,15,20,25,50]\n",
    "total_true=[]\n",
    "total_preds=[[] for _ in range(len(deltas))]\n",
    "for videoname,(y_pred_aus,indices,y_true) in test_videos.items():\n",
    "    for i,ind in enumerate(indices):\n",
    "        if min(y_true[i])>=0:\n",
    "            total_true.append(y_true[i])\n",
    "    cur_ind=0\n",
    "    preds=[]\n",
    "    for i in range(indices[-1]):\n",
    "        if indices[cur_ind]-1==i:\n",
    "            preds.append(y_pred_aus[cur_ind])\n",
    "            cur_ind+=1\n",
    "        else:\n",
    "            if cur_ind==0:\n",
    "                preds.append(y_pred_aus[cur_ind])\n",
    "            else:\n",
    "                w=(i-indices[cur_ind-1]+1)/(indices[cur_ind]-indices[cur_ind-1])\n",
    "                y_pred_aus = np.array(y_pred_aus)\n",
    "                pred=w*y_pred_aus[cur_ind-1]+(1-w)*y_pred_aus[cur_ind]\n",
    "                preds.append(pred)\n",
    "    \n",
    "    preds=np.array(preds)\n",
    "    for hInd,delta in enumerate(deltas):\n",
    "        cur_preds=[]\n",
    "        for i in range(len(preds)):\n",
    "            i1=max(i-delta,0)\n",
    "            pred=np.mean(preds[i1:i+delta+1],axis=0)\n",
    "            aus=(pred>=thresh)*1\n",
    "            #aus=(pred>=0.5)*1\n",
    "            cur_preds.append(aus)\n",
    "        for i,ind in enumerate(indices):\n",
    "            if min(y_true[i])>=0:\n",
    "                total_preds[hInd].append(cur_preds[ind-1])\n",
    "    \n",
    "\n",
    "total_true=np.array(total_true)\n",
    "for hInd,delta in enumerate(deltas):\n",
    "    preds=np.array(total_preds[hInd])\n",
    "    f1scores=[f1_score(y_true=total_true[:,i],y_pred=preds[:,i]) for i in range(preds.shape[1])]\n",
    "    print(delta,np.mean(f1scores),f1scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729736\n"
     ]
    }
   ],
   "source": [
    "test_set = []\n",
    "\n",
    "def get_names(dirname):\n",
    "    \n",
    "    names = []\n",
    "    with open(os.path.join(dirname)) as f:\n",
    "        lines = f.read().splitlines()\n",
    "        \n",
    "        for i,line in enumerate(lines):\n",
    "            if i>0:\n",
    "                name = line[:-1]\n",
    "                names.append(name)\n",
    "                \n",
    "    print(len(names))\n",
    "    return names\n",
    "\n",
    "test_set = get_names(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "718153"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keys = data.keys()\n",
    "c = 0\n",
    "missed = []\n",
    "\n",
    "for k in test_set:\n",
    "    if k in keys:\n",
    "        c+=1\n",
    "        \n",
    "    else:\n",
    "        missed.append(k)\n",
    "        \n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "729737 ['image_location,AU1,AU2,AU4,AU6,AU7,AU10,AU12,AU15,AU23,AU24,AU25,AU26', '2-30-640x360/00001.jpg,', '2-30-640x360/00002.jpg,', '2-30-640x360/00003.jpg,', '2-30-640x360/00004.jpg,']\n",
      "141\n"
     ]
    }
   ],
   "source": [
    "with open(os.path.join(test_dir),'r') as f:\n",
    "    test_set_sample=f.read().splitlines()\n",
    "print(len(test_set_sample),test_set_sample[:5])\n",
    "\n",
    "test_set_videos={}\n",
    "for s in test_set_sample[1:]:\n",
    "    videoname,img_name=s[:-1].split('/')\n",
    "    if videoname not in test_set_videos:\n",
    "        test_set_videos[videoname]=[]\n",
    "    test_set_videos[videoname].append(img_name)\n",
    "    \n",
    "print(len(test_set_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:07<00:00,  2.09it/s]\n"
     ]
    }
   ],
   "source": [
    "datasets = {}\n",
    "for k in tqdm(test_set_videos.keys()):\n",
    "    w2v2large_t, openl3_t, w2v2hub_t = [], [], []\n",
    "    X_t = []\n",
    "    for images in test_set_videos[k]:\n",
    "        key = f'{k}/{images}'\n",
    "        if key in missed: continue\n",
    "        X_t.append(data[key]['frame'][0])\n",
    "#         w2v2large_t.append(data[key]['w2v2large'])\n",
    "#         openl3_t.append(data[key]['openl3'])\n",
    "#         w2v2hub_t.append(data[key]['w2v2hub'])\n",
    "    \n",
    "    a_dataset = audioDataset(['frames'],\n",
    "                     [X_t], window=300, step=300)\n",
    "    training_loader = DataLoader(a_dataset, batch_size=64, shuffle=False)\n",
    "    datasets[k] = training_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(loader, model):\n",
    "    pred_labels_val = []\n",
    "#     predicts = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(loader):\n",
    "            vinputs, d_len = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            pred_labels_val += voutputs.data.tolist()\n",
    "            \n",
    "#             _, predicted = torch.max(voutputs.data, 1)\n",
    "#             predicts += predicted.tolist()\n",
    "    \n",
    "    d_len = d_len[0]\n",
    "    s = 300 - len(pred_labels_val) + d_len\n",
    "    return pred_labels_val[:-300]+pred_labels_val[-s:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 141/141 [01:29<00:00,  1.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "test_videos={}\n",
    "test_videos_num_frames={}\n",
    "for videoname,img_files in tqdm(test_set_videos.items()):\n",
    "    X,indices,filenames,scores=[],[],[],[]\n",
    "    num_present=num_missed=0\n",
    "    for img_name in img_files:\n",
    "        k=videoname+'/'+img_name\n",
    "        if k in data:\n",
    "            indices.append(int(img_name[:-4]))\n",
    "#             print()\n",
    "            filenames.append(k)\n",
    "            num_present+=1\n",
    "        else:\n",
    "            num_missed+=1\n",
    "    test_videos[videoname]=(predict(datasets[videoname], model),indices,filenames)\n",
    "    test_videos_num_frames[videoname]=(num_present,num_missed)\n",
    "    \n",
    "#     del datasets[videoname]\n",
    "#     torch.cuda.empty_cache()\n",
    "    \n",
    "print(len(test_videos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_au_results(res_filename):\n",
    "    with open(os.path.join(res_filename), 'w') as f:\n",
    "        f.write(test_set_sample[0]+'\\n')\n",
    "        for videoname,(y_pred_au,indices,filenames) in test_videos.items():\n",
    "            cur_ind=0\n",
    "            preds=[]\n",
    "            for i in range(indices[-1]):\n",
    "                if indices[cur_ind]-1==i:\n",
    "                    preds.append(y_pred_au[cur_ind])\n",
    "                    cur_ind+=1\n",
    "                else:\n",
    "                    if cur_ind==0:\n",
    "                        preds.append(y_pred_au[cur_ind])\n",
    "                    else:\n",
    "                        w=(i-indices[cur_ind-1]+1)/(indices[cur_ind]-indices[cur_ind-1])\n",
    "                        y_pred_au = np.array(y_pred_au)\n",
    "                        pred=w*y_pred_au[cur_ind-1]+(1-w)*y_pred_au[cur_ind]\n",
    "                        preds.append(pred)\n",
    "\n",
    "            pred=y_pred_au[cur_ind-1]\n",
    "            for _ in range(indices[-1],len(test_set_videos[videoname])):\n",
    "                preds.append(pred)\n",
    "\n",
    "            preds=np.array(preds)\n",
    "            for i,img_name in enumerate(test_set_videos[videoname]):\n",
    "                i1=max(i-delta,0)\n",
    "                pred=np.mean(preds[i1:i+delta+1],axis=0)\n",
    "                aus=(pred>=thresh)*1\n",
    "                f.write(videoname+'/'+img_name+','+','.join(map(str,aus))+'\\n')\n",
    "delta = 3\n",
    "res_filename='au_predictions/predictions_au_tcn_only_video.txt'\n",
    "write_au_results(res_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
