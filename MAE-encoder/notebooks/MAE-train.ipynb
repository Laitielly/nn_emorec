{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f0d5981",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47be43ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d5dc238",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.0.1+cu117\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from random import shuffle\n",
    "from PIL import Image\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import glob\n",
    "import gc\n",
    "from itertools import chain\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8caa4c12",
   "metadata": {},
   "outputs": [],
   "source": [
    "from comet_ml import Experiment, ExistingExperiment\n",
    "from comet_ml.integration.pytorch import log_model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a3c76ee",
   "metadata": {},
   "source": [
    "# Get model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0a2f7834",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "43406717"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_paths = None\n",
    "with open('images/MAE_train.txt', 'r') as file:\n",
    "    train_paths = file.readlines()\n",
    "    \n",
    "len(train_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970e18b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "80117"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_paths = None\n",
    "with open('images/MAE_val.txt', 'r') as file:\n",
    "    val_paths = file.readlines()\n",
    "    \n",
    "len(val_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "54801b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE=224\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.533, 0.425, 0.374],\n",
    "                             std=[0.244, 0.214, 0.202])\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "08856278",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_images(transform, imgs):\n",
    "    img_tensors = []\n",
    "    for img_name in imgs:\n",
    "        try:\n",
    "            img_name = img_name.replace('\\n', '')\n",
    "            img = Image.open(img_name)\n",
    "            img_tensor = transform(img)\n",
    "            if img.size:\n",
    "                img_tensors.append(img_tensor)\n",
    "\n",
    "        except:\n",
    "            print(f'{img_name} does not open')\n",
    "            \n",
    "    try:\n",
    "        img_tensors = torch.stack(img_tensors)\n",
    "            \n",
    "    except: \n",
    "        img_tensors = []\n",
    "        \n",
    "    return img_tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f11ecc84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mae.encoder import ViTBaseEncoder\n",
    "from mae.mae import MAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7404929c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(args):\n",
    "    '''\n",
    "    build MAE model.\n",
    "    :param args: model args\n",
    "    :return: model\n",
    "    '''\n",
    "    # build model\n",
    "    v = ViTBaseEncoder(image_size=args['image_size'],\n",
    "                       patch_size=args['patch_size'],\n",
    "                       dim=args['vit_dim'],\n",
    "                       depth=args['vit_depth'],\n",
    "                       heads=args['vit_heads'],\n",
    "                       mlp_dim=args['vit_mlp_dim'],\n",
    "                       masking_ratio=args['masking_ratio'],\n",
    "                       device=args['device']).to(args['device'])\n",
    "\n",
    "    mae = MAE(encoder=v,\n",
    "              decoder_dim=args['decoder_dim'],\n",
    "              decoder_depth=args['decoder_depth'],\n",
    "              device=args['device']).to(args['device'])\n",
    "\n",
    "    return mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a2c6cbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'image_size': IMG_SIZE,\n",
    "    'patch_size': 16,\n",
    "    'vit_dim': 768,\n",
    "    'vit_depth': 5,\n",
    "    'vit_heads': 6,\n",
    "    'vit_mlp_dim': 2048,\n",
    "    'masking_ratio': 0.75,\n",
    "    'decoder_dim': 256,\n",
    "    'decoder_depth': 5,\n",
    "    'device': 'cuda'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ceed2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cpath = '/home/hse_student/apsidorova/embedding_models/mae/mae/ckpt/EMERGY_Vit_Base_ep1_step53820.pt'\n",
    "model = build_model(args)\n",
    "model.train();\n",
    "model.load_state_dict(torch.load(cpath))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ebdb4",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e1266913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# experiment = Experiment(\n",
    "#   api_key=\"XhQqrLR91F7zW3AZ7LgVT3zp2\",\n",
    "#   project_name=\"abaw6\",\n",
    "#   workspace=\"annanet\"\n",
    "# )\n",
    "\n",
    "# experiment.set_name('MAE train')\n",
    "# experiment.add_tags(['AffectNet', 'CASIA-WebFace', 'CelebA', 'IMDB-WIKI', 'WebFace260M'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "43557201",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m As you are running in a Jupyter environment, you will need to call `experiment.end()` when finished to ensure all metrics and code are logged before exiting.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Couldn't find a Git repository in '/home/hse_student/apsidorova/embedding_models/mae' nor in any parent directory. Set `COMET_GIT_DIRECTORY` if your Git Repository is elsewhere.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Experiment is live on comet.com https://www.comet.com/annanet/abaw6/b9a8283da29c4831a4def0cb5fea0a8a\n",
      "\n"
     ]
    }
   ],
   "source": [
    "experiment = ExistingExperiment(\n",
    "        api_key=\"XhQqrLR91F7zW3AZ7LgVT3zp2\",\n",
    "        experiment_key=\"b9a8283da29c4831a4def0cb5fea0a8a\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50ab6bde",
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparams = {\n",
    "    'epochs': 10,\n",
    "    'optimizer': 'AdamW',\n",
    "    'loss': 'pixel-wise L2 loss',\n",
    "    'lr': 1.5e-5,\n",
    "    'steplr': 1,\n",
    "    'batch': 256,\n",
    "    'weight_decay': 5e-2,\n",
    "    'momentum': (0.9, 0.95),\n",
    "    'epochs_warmup': 40,\n",
    "    'warmup_from': 1e-3, \n",
    "    'lr_decay_rate': 1e-2,\n",
    "    'warmup_to': 0.0002981531029360196,\n",
    "    'ckpt_folder_best': '/home/hse_student/apsidorova/embedding_models/mae/mae/ckpt_best',\n",
    "    'ckpt_folder': '/home/hse_student/apsidorova/embedding_models/mae/mae/ckpt',\n",
    "    'freq_val': 1_000,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7bfeb4ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.log_parameters(hyperparams)\n",
    "experiment.log_parameters(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f7f86fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainp_loader = DataLoader(train_paths,\n",
    "                           batch_size=hyperparams['batch'], \n",
    "                           shuffle=True)\n",
    "valp_loader = DataLoader(val_paths,\n",
    "                         batch_size=hyperparams['batch'], \n",
    "                         shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1e835473",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=hyperparams['lr'],\n",
    "                              weight_decay=hyperparams['weight_decay'],\n",
    "                              betas=hyperparams['momentum'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ab1234a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def lr_lambda(epoch):\n",
    "    if epoch < hyperparams['steplr']:\n",
    "        lr =  1.0\n",
    "    elif epoch < hyperparams['epochs_warmup']:\n",
    "        p = epoch / hyperparams['epochs_warmup']\n",
    "        lr = hyperparams['warmup_from'] + p * (hyperparams['warmup_to'] - hyperparams['warmup_from'])\n",
    "    else:\n",
    "        eta_min = hyperparams['lr'] * (hyperparams['lr_decay_rate'] ** 3)\n",
    "        lr = eta_min + (hyperparams['lr'] - eta_min) * (1 + math.cos(math.pi * epoch / hyperparams['epochs'])) / 2\n",
    "    return lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf06959c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=lr_lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "364be50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    '''\n",
    "    compute and store the average and current value\n",
    "    '''\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5cdb473",
   "metadata": {},
   "outputs": [],
   "source": [
    "global_best_loss = 1e+10\n",
    "checkpoint_freq = 100\n",
    "PATH = ''\n",
    "\n",
    "for epoch in range(1, hyperparams['epochs'] + 1):\n",
    "    # records\n",
    "    losses = AverageMeter()\n",
    "    print('\\nEPOCH {}:'.format(epoch))\n",
    "\n",
    "    print('Start train')\n",
    "    # train by epoch\n",
    "    try:\n",
    "        for idx, path in tqdm(enumerate(trainp_loader), total=len(trainp_loader)):\n",
    "            # put images into device\n",
    "            tensor = get_images(transform, path)\n",
    "\n",
    "            if tensor==[]:\n",
    "                print('Do not find images')\n",
    "                continue\n",
    "\n",
    "            tensor = tensor.to(args['device'])\n",
    "            # forward\n",
    "            loss = model(tensor)\n",
    "            experiment.log_metric('current loss train', loss.to('cpu').item(), \n",
    "                                  step=(epoch-1)*len(trainp_loader) + idx)\n",
    "            # back propagation\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            # record\n",
    "            losses.update(loss.to('cpu').item(), hyperparams['batch'])\n",
    "            del tensor\n",
    "            del loss\n",
    "            del path\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            optimizer.step()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "            if (idx%checkpoint_freq)==0:\n",
    "                print('Saving checkpoint')\n",
    "                cpath = f'{hyperparams[\"ckpt_folder\"]}/Vit_Base_ep{epoch}_step{idx}.pt'\n",
    "                torch.save(model.state_dict(), cpath)\n",
    "\n",
    "            if (idx%hyperparams['freq_val'])==0:\n",
    "                model.eval()\n",
    "\n",
    "                losses_val = AverageMeter()\n",
    "                print('Val part')\n",
    "\n",
    "                for path in tqdm(valp_loader, total=len(valp_loader)):\n",
    "                    tensor = get_images(transform, path).to(args['device'])\n",
    "                    if tensor==[]:\n",
    "                        print('Do not find images')\n",
    "                        continue\n",
    "                    loss = model(tensor)\n",
    "                    # record\n",
    "                    losses_val.update(loss.to('cpu').item(), hyperparams['batch'])\n",
    "                    del tensor\n",
    "                    del loss\n",
    "                    del path\n",
    "                    gc.collect()\n",
    "                    torch.cuda.empty_cache()\n",
    "\n",
    "                experiment.log_metric('avg loss val', losses_val.avg, \n",
    "                                      step=(epoch-1)*len(trainp_loader) + idx)\n",
    "                print(f'Current Validation loss is {losses_val.avg}')\n",
    "                if global_best_loss > losses_val.avg:\n",
    "                    global_best_loss = losses_val.avg\n",
    "                    print('New Best Validation loss')\n",
    "\n",
    "                    # save model\n",
    "                    PATH = f'{hyperparams[\"ckpt_folder_best\"]}/Vit_Base_ep{epoch}_step{idx}.pt'\n",
    "                    torch.save(model, PATH)\n",
    "                model.train()\n",
    "\n",
    "            experiment.log_metric('avg loss train', losses.avg, \n",
    "                                  epoch=epoch)\n",
    "            print(f'Current Train average loss is {losses.avg}')\n",
    "\n",
    "        scheduler.step()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(type(e).__name__)  \n",
    "        print('EMERGY saving checkpoint')\n",
    "        cpath = f'{hyperparams[\"ckpt_folder\"]}/EMERGY_Vit_Base_ep{epoch}_step{idx}.pt'\n",
    "        torch.save(model.state_dict(), cpath)\n",
    "        \n",
    "        torch.save(trainp_loader, 'EMERGY_train_dataloader.pth')\n",
    "        torch.save(valp_loader, 'EMERGY_val_dataloader.pth')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "33808b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml ExistingExperiment Summary\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/annanet/abaw6/b9a8283da29c4831a4def0cb5fea0a8a\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     avg loss train [763]     : (0.12267985567450523, 0.12730881253655038)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     avg loss val             : 0.14919541102533523\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     current loss train [764] : (0.1112850233912468, 0.14719419181346893)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch            : 256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ckpt_folder      : /home/hse_student/apsidorova/embedding_models/mae/mae/ckpt\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     ckpt_folder_best : /home/hse_student/apsidorova/embedding_models/mae/mae/ckpt_best\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_depth    : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     decoder_dim      : 256\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     device           : cuda\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs           : 10\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     epochs_warmup    : 40\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     freq_val         : 1000\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     image_size       : 224\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     loss             : pixel-wise L2 loss\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr               : 1.5e-05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lr_decay_rate    : 0.01\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     masking_ratio    : 0.75\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     momentum         : (0.9, 0.95)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     optimizer        : AdamW\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     patch_size       : 16\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     steplr           : 1\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vit_depth        : 5\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vit_dim          : 768\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vit_heads        : 6\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     vit_mlp_dim      : 2048\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_from      : 0.001\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     warmup_to        : 0.0002981531029360196\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay     : 0.05\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     model-element : 1 (107.28 MB)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
      "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: torch, sklearn.\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for metadata to finish uploading (timeout is 3600 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Uploading 41 metrics, params and output messages\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Please wait for assets to finish uploading (timeout is 10800 seconds)\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 file(s), remaining 107.28 MB/107.28 MB\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 97.28 MB/107.28 MB, Throughput 681.99 KB/s, ETA ~147s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 87.28 MB/107.28 MB, Throughput 681.85 KB/s, ETA ~132s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 72.28 MB/107.28 MB, Throughput 1022.97 KB/s, ETA ~73s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 57.28 MB/107.28 MB, Throughput 1022.88 KB/s, ETA ~58s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 47.28 MB/107.28 MB, Throughput 681.87 KB/s, ETA ~72s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 32.28 MB/107.28 MB, Throughput 1022.83 KB/s, ETA ~33s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 17.28 MB/107.28 MB, Throughput 1022.85 KB/s, ETA ~18s\n",
      "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Still uploading 1 asset(s), remaining 7.28 MB/107.28 MB, Throughput 682.02 KB/s, ETA ~11s\n"
     ]
    }
   ],
   "source": [
    "experiment.log_model(\"mae_model.pt\", file_or_folder=PATH)\n",
    "experiment.end()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
