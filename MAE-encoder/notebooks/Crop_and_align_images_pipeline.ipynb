{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:03.407149Z",
     "iopub.status.busy": "2024-05-06T21:41:03.406307Z",
     "iopub.status.idle": "2024-05-06T21:41:03.594000Z",
     "shell.execute_reply": "2024-05-06T21:41:03.593135Z",
     "shell.execute_reply.started": "2024-05-06T21:41:03.407114Z"
    },
    "id": "kkwv9AwgZPSE"
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:03.596593Z",
     "iopub.status.busy": "2024-05-06T21:41:03.596288Z",
     "iopub.status.idle": "2024-05-06T21:41:09.702974Z",
     "shell.execute_reply": "2024-05-06T21:41:09.702050Z",
     "shell.execute_reply.started": "2024-05-06T21:41:03.596569Z"
    },
    "id": "AebSwGm0ZUPM"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "from retinaface.pre_trained_models import get_model\n",
    "from retinaface.utils import vis_annotations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:09.704663Z",
     "iopub.status.busy": "2024-05-06T21:41:09.704272Z",
     "iopub.status.idle": "2024-05-06T21:41:09.709280Z",
     "shell.execute_reply": "2024-05-06T21:41:09.707764Z",
     "shell.execute_reply.started": "2024-05-06T21:41:09.704637Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Functions & model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:09.711427Z",
     "iopub.status.busy": "2024-05-06T21:41:09.710788Z",
     "iopub.status.idle": "2024-05-06T21:41:09.719740Z",
     "shell.execute_reply": "2024-05-06T21:41:09.718818Z",
     "shell.execute_reply.started": "2024-05-06T21:41:09.711390Z"
    }
   },
   "outputs": [],
   "source": [
    "def is_specialfile(path,exts):\n",
    "    _, file_extension = os.path.splitext(path)\n",
    "    return file_extension.lower() in exts\n",
    "\n",
    "img_extensions=['.jpg','.jpeg','.png']\n",
    "def is_image(path):\n",
    "    return is_specialfile(path,img_extensions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:09.721328Z",
     "iopub.status.busy": "2024-05-06T21:41:09.720965Z",
     "iopub.status.idle": "2024-05-06T21:41:14.229556Z",
     "shell.execute_reply": "2024-05-06T21:41:14.228727Z",
     "shell.execute_reply.started": "2024-05-06T21:41:09.721301Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hse_student/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/hse_student/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "/home/hse_student/.local/lib/python3.8/site-packages/torch/hub.py:665: UserWarning: Falling back to the old format < 1.6. This support will be deprecated in favor of default zipfile format introduced in 1.6. Please redo torch.save() to save it in the new zipfile format.\n",
      "  warnings.warn('Falling back to the old format < 1.6. This support will be '\n"
     ]
    }
   ],
   "source": [
    "from retinaface.pre_trained_models import get_model\n",
    "model = get_model(\"resnet50_2020-07-20\", max_size=1024, device='cuda')\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:14.231849Z",
     "iopub.status.busy": "2024-05-06T21:41:14.230732Z",
     "iopub.status.idle": "2024-05-06T21:41:14.245980Z",
     "shell.execute_reply": "2024-05-06T21:41:14.244654Z",
     "shell.execute_reply.started": "2024-05-06T21:41:14.231809Z"
    }
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from skimage import transform as trans\n",
    "\n",
    "def preprocess(img, bbox=None, landmark=None, **kwargs):\n",
    "    M = None\n",
    "    image_size = [224,224]\n",
    "    src = np.array([\n",
    "      [30.2946, 51.6963],\n",
    "      [65.5318, 51.5014],\n",
    "      [48.0252, 71.7366],\n",
    "      [33.5493, 92.3655],\n",
    "      [62.7299, 92.2041] ], dtype=np.float32 )\n",
    "    if image_size[1]==224:\n",
    "        src[:,0] += 8.0\n",
    "    src*=2\n",
    "    if landmark is not None:\n",
    "        dst = landmark.astype(np.float32)\n",
    "\n",
    "        tform = trans.SimilarityTransform()\n",
    "        tform.estimate(dst, src)\n",
    "        M = tform.params[0:2,:]\n",
    "\n",
    "    if M is None:\n",
    "        if bbox is None: #use center crop\n",
    "            det = np.zeros(4, dtype=np.int32)\n",
    "            det[0] = int(img.shape[1]*0.0625)\n",
    "            det[1] = int(img.shape[0]*0.0625)\n",
    "            det[2] = img.shape[1] - det[0]\n",
    "            det[3] = img.shape[0] - det[1]\n",
    "        else:\n",
    "              det = bbox\n",
    "        margin = kwargs.get('margin', 44)\n",
    "        bb = np.zeros(4, dtype=np.int32)\n",
    "        bb[0] = np.maximum(det[0]-margin//2, 0)\n",
    "        bb[1] = np.maximum(det[1]-margin//2, 0)\n",
    "        bb[2] = np.minimum(det[2]+margin//2, img.shape[1])\n",
    "        bb[3] = np.minimum(det[3]+margin//2, img.shape[0])\n",
    "        ret = img[bb[1]:bb[3],bb[0]:bb[2],:]\n",
    "        if len(image_size)>0:\n",
    "              ret = cv2.resize(ret, (image_size[1], image_size[0]))\n",
    "        return ret \n",
    "    else: #do align using landmark\n",
    "        assert len(image_size)==2\n",
    "        warped = cv2.warpAffine(img,M,(image_size[1],image_size[0]), borderValue = 0.0)\n",
    "        return warped"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ð¡reating destination folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:17.302596Z",
     "iopub.status.busy": "2024-05-06T21:41:17.302224Z",
     "iopub.status.idle": "2024-05-06T21:41:17.307556Z",
     "shell.execute_reply": "2024-05-06T21:41:17.306478Z",
     "shell.execute_reply.started": "2024-05-06T21:41:17.302561Z"
    }
   },
   "outputs": [],
   "source": [
    "# example on CASIA-WebFace\n",
    "path_dir = '/home/hse_student/apsidorova/dataset_mae/faces_webface_112x112/unbin_images'\n",
    "outfile_cropped = 'CASIA-WebFace/cropped'\n",
    "outfile_aligned = 'CASIA-WebFace/aligned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "!mkdir CASIA-WebFace\n",
    "!mkdir $outfile_cropped\n",
    "!mkdir $outfile_aligned\n",
    "for folder in os.listdir(path_dir):\n",
    "    f = os.path.join(path_dir, folder)\n",
    "    if os.path.isdir(f):\n",
    "        !mkdir $outfile_cropped/$folder\n",
    "        !mkdir $outfile_aligned/$folder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crop and Align images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-06T21:41:29.473909Z",
     "iopub.status.busy": "2024-05-06T21:41:29.473497Z",
     "iopub.status.idle": "2024-05-07T08:17:16.989668Z",
     "shell.execute_reply": "2024-05-07T08:17:16.988546Z",
     "shell.execute_reply.started": "2024-05-06T21:41:29.473869Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for folder in os.listdir(path_dir):\n",
    "    f = os.path.join(path_dir, folder)\n",
    "    print(f)\n",
    "    if os.path.isdir(f):\n",
    "        for image_name in tqdm(os.listdir(f)):\n",
    "            path_image = os.path.join(f, image_name)\n",
    "            if is_image(image_name):\n",
    "                image = cv2.imread(path_image)\n",
    "                annotations = model.predict_jsons(image)\n",
    "                \n",
    "                if not annotations[0][\"bbox\"]:\n",
    "                    print('No faces')\n",
    "                else:\n",
    "                    for annotation in annotations:\n",
    "                        box = np.array(annotation['bbox']).astype(int)\n",
    "                        x1,y1,x2,y2=box[0:4]\n",
    "                        x1=max(x1,0)\n",
    "                        y1=max(y1,0)\n",
    "                        face_img=image[y1:y2,x1:x2,:]\n",
    "                        p=np.array(annotation['landmarks'])\n",
    "\n",
    "                        cv2.imwrite(os.path.join(outfile_cropped, \n",
    "                                                 folder,\n",
    "                                                 image_name), \n",
    "                                    face_img) \n",
    "\n",
    "                        face_img=preprocess(image,box,p)\n",
    "                        cv2.imwrite(os.path.join(outfile_aligned, \n",
    "                                                 folder,\n",
    "                                                 image_name), \n",
    "                                    face_img) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 4948980,
     "sourceId": 8333816,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30698,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
