{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://huggingface.co/audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-02-12T13:01:17.088741Z",
     "iopub.status.busy": "2024-02-12T13:01:17.088472Z",
     "iopub.status.idle": "2024-02-12T13:01:24.151431Z",
     "shell.execute_reply": "2024-02-12T13:01:24.150497Z",
     "shell.execute_reply.started": "2024-02-12T13:01:17.088716Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import inspect\n",
    "from math import floor\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import librosa\n",
    "from transformers import Wav2Vec2Processor\n",
    "from transformers.models.wav2vec2.modeling_wav2vec2 import (\n",
    "    Wav2Vec2Model,\n",
    "    Wav2Vec2PreTrainedModel,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:01:24.153666Z",
     "iopub.status.busy": "2024-02-12T13:01:24.153241Z",
     "iopub.status.idle": "2024-02-12T13:01:50.084698Z",
     "shell.execute_reply": "2024-02-12T13:01:50.083776Z",
     "shell.execute_reply.started": "2024-02-12T13:01:24.153640Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "37b47c7006c944d8814ced49c5423acd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/214 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39f62291db4146f888b631476db61040",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/2.34k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09f3da1e36ed427b92f193eb69afd8fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.00 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f55ba732dc1d49c99380d75cbaac65af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/661M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EmotionModel were not initialized from the model checkpoint at audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim and are newly initialized: ['wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original0', 'wav2vec2.encoder.pos_conv_embed.conv.parametrizations.weight.original1']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "2024-02-12 13:01:40.209332: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-02-12 13:01:40.209450: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-02-12 13:01:40.333484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1024)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RegressionHead(nn.Module):\n",
    "    r\"\"\"Classification head.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
    "        self.dropout = nn.Dropout(config.final_dropout)\n",
    "        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n",
    "\n",
    "    def forward(self, features, **kwargs):\n",
    "\n",
    "        x = features\n",
    "        x = self.dropout(x)\n",
    "        x = self.dense(x)\n",
    "        x = torch.tanh(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.out_proj(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class EmotionModel(Wav2Vec2PreTrainedModel):\n",
    "    r\"\"\"Speech emotion classifier.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "\n",
    "        super().__init__(config)\n",
    "\n",
    "        self.config = config\n",
    "        self.wav2vec2 = Wav2Vec2Model(config).to(device)\n",
    "        self.classifier = RegressionHead(config).to(device)\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(\n",
    "            self,\n",
    "            input_values,\n",
    "    ):\n",
    "\n",
    "        outputs = self.wav2vec2(input_values)\n",
    "        hidden_states = outputs[0]\n",
    "        hidden_states = torch.mean(hidden_states, dim=1)\n",
    "        logits = self.classifier(hidden_states)\n",
    "\n",
    "        return hidden_states, logits\n",
    "\n",
    "\n",
    "\n",
    "device = 'cuda'\n",
    "model_name = 'audeering/wav2vec2-large-robust-12-ft-emotion-msp-dim'\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name)\n",
    "model = EmotionModel.from_pretrained(model_name)\n",
    "\n",
    "# dummy signal\n",
    "sampling_rate = 16000\n",
    "signal = np.zeros((1, sampling_rate), dtype=np.float32)\n",
    "\n",
    "\n",
    "def process_func(\n",
    "    x: np.ndarray,\n",
    "    sampling_rate: int,\n",
    "    embeddings: bool = False,\n",
    ") -> np.ndarray:\n",
    "    r\"\"\"Predict emotions or extract embeddings from raw audio signal.\"\"\"\n",
    "\n",
    "    # run through processor to normalize signal\n",
    "    # always returns a batch, so we just get the first entry\n",
    "    # then we put it on the device\n",
    "    y = processor(x, sampling_rate=sampling_rate)\n",
    "    y = y['input_values'][0]\n",
    "    y = y.reshape(1, -1)\n",
    "    y = torch.from_numpy(y).to(device)\n",
    "\n",
    "    # run through model\n",
    "    with torch.no_grad():\n",
    "        y = model(y)[0 if embeddings else 1]\n",
    "\n",
    "    # convert to numpy\n",
    "    y = y.detach().cpu().numpy()\n",
    "\n",
    "    return y\n",
    "\n",
    "\n",
    "audio, sr = librosa.load('/kaggle/input/audio-abaw5/batch1/batch1/108-15-640x480.mp3', sr=16000)\n",
    "l = process_func(audio, sr, embeddings=True)\n",
    "l.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:01:50.086685Z",
     "iopub.status.busy": "2024-02-12T13:01:50.085917Z",
     "iopub.status.idle": "2024-02-12T13:01:50.091985Z",
     "shell.execute_reply": "2024-02-12T13:01:50.090889Z",
     "shell.execute_reply.started": "2024-02-12T13:01:50.086652Z"
    }
   },
   "outputs": [],
   "source": [
    "data_dir = '/kaggle/input/audio-abaw5'\n",
    "folders = ['batch1', 'batch2', 'new_vids']\n",
    "\n",
    "names = []\n",
    "global_features = []\n",
    "\n",
    "step = 24000 #1.5sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:01:50.094989Z",
     "iopub.status.busy": "2024-02-12T13:01:50.094486Z",
     "iopub.status.idle": "2024-02-12T13:01:50.127428Z",
     "shell.execute_reply": "2024-02-12T13:01:50.126414Z",
     "shell.execute_reply.started": "2024-02-12T13:01:50.094956Z"
    }
   },
   "outputs": [],
   "source": [
    "def process_signal(local_name, audio, step=24000, sr=16000):\n",
    "    names = []\n",
    "    features = []\n",
    "    th = floor(audio.shape[0] / step)\n",
    "\n",
    "    for s in range(th):\n",
    "        with torch.no_grad():\n",
    "            signal = process_func(audio[step*s:(s+1)*step], sr, embeddings=True)\n",
    "        features.append(signal[0])\n",
    "        names.append(f'{local_name}/{str(s+1).zfill(5)}')\n",
    "\n",
    "    if audio[step*th:].shape[0] > 0:\n",
    "        new_step = step - audio[step*th:].shape[0]\n",
    "        with torch.no_grad():\n",
    "            signal = process_func(audio[step*th - new_step:], sr, embeddings=True)\n",
    "        features.append(signal[0])\n",
    "        names.append(f'{local_name}/{str(th+1).zfill(5)}')\n",
    "        \n",
    "    return names, features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:01:50.129106Z",
     "iopub.status.busy": "2024-02-12T13:01:50.128706Z",
     "iopub.status.idle": "2024-02-12T13:24:43.163911Z",
     "shell.execute_reply": "2024-02-12T13:24:43.162937Z",
     "shell.execute_reply.started": "2024-02-12T13:01:50.129045Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in batch1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [16:47<00:00,  2.12s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in batch2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 73/73 [02:27<00:00,  2.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in new_vids\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [03:37<00:00,  4.35s/it]\n"
     ]
    }
   ],
   "source": [
    "for folder in folders:\n",
    "    dirpath=os.path.join(data_dir, folder, folder)\n",
    "    print(f'in {folder}')\n",
    "\n",
    "    for filename in tqdm(os.listdir(dirpath)):\n",
    "        fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "        if ext.lower()=='.mp3':\n",
    "            local_name = f'{fn}'\n",
    "            \n",
    "            audio, sr = librosa.load(os.path.join(dirpath, filename), sr=16000)\n",
    "            nn, fea = process_signal(local_name, audio, step)\n",
    "            \n",
    "            names += nn\n",
    "            \n",
    "            if len(global_features):\n",
    "                global_features=np.concatenate((global_features, fea),axis=0)\n",
    "            else:\n",
    "                global_features = fea\n",
    "                \n",
    "        else:\n",
    "            print(filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:24:43.165533Z",
     "iopub.status.busy": "2024-02-12T13:24:43.165212Z",
     "iopub.status.idle": "2024-02-12T13:24:43.171917Z",
     "shell.execute_reply": "2024-02-12T13:24:43.170961Z",
     "shell.execute_reply.started": "2024-02-12T13:24:43.165506Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((73460, 1024), 73460)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "global_features.shape, len(names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:24:43.173591Z",
     "iopub.status.busy": "2024-02-12T13:24:43.173240Z",
     "iopub.status.idle": "2024-02-12T13:24:44.207174Z",
     "shell.execute_reply": "2024-02-12T13:24:44.206389Z",
     "shell.execute_reply.started": "2024-02-12T13:24:43.173559Z"
    }
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "filename2featuresAll={img_name: gl_feature for img_name, gl_feature \n",
    "                      in zip(names, global_features)}\n",
    "\n",
    "with open('wav2vec_large_robast_fea.pickle', 'wb') as handle:\n",
    "    pickle.dump(filename2featuresAll, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-02-12T13:24:44.208939Z",
     "iopub.status.busy": "2024-02-12T13:24:44.208478Z",
     "iopub.status.idle": "2024-02-12T13:24:44.215535Z",
     "shell.execute_reply": "2024-02-12T13:24:44.214706Z",
     "shell.execute_reply.started": "2024-02-12T13:24:44.208898Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<a href='wav2vec_large_robast_fea.pickle' target='_blank'>wav2vec_large_robast_fea.pickle</a><br>"
      ],
      "text/plain": [
       "/kaggle/working/wav2vec_large_robast_fea.pickle"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import FileLink\n",
    "\n",
    "FileLink('wav2vec_large_robast_fea.pickle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 4283660,
     "sourceId": 7372463,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30646,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
