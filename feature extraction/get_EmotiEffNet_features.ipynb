{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: CUDA_VISIBLE_DEVICES=1\n"
     ]
    }
   ],
   "source": [
    "%env CUDA_VISIBLE_DEVICES=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Torch: 2.0.1+cu118\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "\n",
    "import time\n",
    "import pickle\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "print(f\"Torch: {torch.__version__}\")\n",
    "device = 'cuda'\n",
    "use_cuda = torch.cuda.is_available()\n",
    "print(use_cuda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'NVIDIA GeForce RTX 2080 Ti'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.get_device_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH='/home/avsavchenko/src/face-emotion-recognition/models/affectnet_emotions/enet_b0_8_best_vgaf.pt'\n",
    "IMG_SIZE=224\n",
    "\n",
    "\n",
    "test_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "np_transforms = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToPILImage(None),\n",
    "        transforms.Resize((IMG_SIZE,IMG_SIZE)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                                     std=[0.229, 0.224, 0.225])\n",
    "    ]\n",
    ")\n",
    "\n",
    "feature_extractor_model = torch.load(PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_weights=feature_extractor_model.classifier[0].weight.cpu().data.numpy()\n",
    "classifier_bias=feature_extractor_model.classifier[0].bias.cpu().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_extractor_model.classifier=torch.nn.Identity()\n",
    "feature_extractor_model=feature_extractor_model.to(device)\n",
    "feature_extractor_model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = '/home/HDD6TB/datasets/emotions/ABAW/ABAW_6/6th_ABAW_Annotations'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "dirnames = ['cropped','cropped_aligned/cropped_aligned_new_50_vids']\n",
    "# dirnames = ['cropped_aligned/cropped_aligned','cropped_aligned/cropped_aligned_new_50_vids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e06c96760054f4db0aeafc220402b3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/594 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d4d38c758ad14c6d8fdcb0104f3cd516",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/31 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "3227429"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l = 0\n",
    "for d in dirnames:\n",
    "    dirpath=os.path.join(DATA_DIR, d)\n",
    "    for folder in tqdm(os.listdir(dirpath)):\n",
    "        if folder=='.DS_Store': continue\n",
    "        for filename in os.listdir(os.path.join(dirpath, folder)):\n",
    "            fn, ext = os.path.splitext(os.path.basename(filename))\n",
    "            if ext.lower()=='.jpg':\n",
    "                l+=1\n",
    "            \n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_probab(features):\n",
    "    x=np.dot(features,np.transpose(classifier_weights))+classifier_bias\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compose(\n",
      "    Resize(size=(224, 224), interpolation=bilinear, max_size=None, antialias=warn)\n",
      "    ToTensor()\n",
      "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(test_transforms)\n",
    "img_names=[]\n",
    "X_global_features=[]\n",
    "imgs=[]\n",
    "\n",
    "MODEL2FEATURES = 'enet_b0_8_best_vgaf_cropped.pickle' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, 0)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(img_names), len(X_global_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in dirnames:\n",
    "    data_dir=os.path.join(DATA_DIR, d)\n",
    "    print(data_dir)\n",
    "    \n",
    "    for _, filename in enumerate(tqdm(os.listdir(data_dir))):\n",
    "        \n",
    "        frames_dir = os.path.join(data_dir, filename)\n",
    "        if '.DS_Store'==filename:\n",
    "            continue\n",
    "        \n",
    "#         if filename in num_files.keys():\n",
    "#             if len(os.listdir(frames_dir)) == num_files[filename]:\n",
    "#                 continue\n",
    "            \n",
    "        for img_name in os.listdir(frames_dir):\n",
    "            \n",
    "            if img_name.lower().endswith('.jpg'):\n",
    "                    \n",
    "                img = Image.open(os.path.join(frames_dir,img_name))\n",
    "                img_tensor = test_transforms(img)\n",
    "\n",
    "                if img.size:\n",
    "\n",
    "                    img_names.append(filename+'/'+img_name)\n",
    "                    imgs.append(img_tensor)\n",
    "\n",
    "                    if len(imgs)>=64:\n",
    "                        features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "                        features=features.data.cpu().numpy()\n",
    "\n",
    "                        if len(X_global_features)==0:\n",
    "                            X_global_features=features\n",
    "                        else:\n",
    "                            X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "                        imgs=[]\n",
    "\n",
    "\n",
    "if len(imgs)>0:\n",
    "    features = feature_extractor_model(torch.stack(imgs, dim=0).to(device))\n",
    "    features=features.data.cpu().numpy()\n",
    "\n",
    "    if len(X_global_features)==0:\n",
    "        X_global_features=features\n",
    "    else:\n",
    "        X_global_features=np.concatenate((X_global_features,features),axis=0)\n",
    "\n",
    "    imgs=[]\n",
    "\n",
    "print(X_global_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scores=get_probab(X_global_features)                \n",
    "filename2featuresAll={img_name:(global_features,scores) for img_name,global_features,scores in zip(img_names,X_global_features,X_scores)}\n",
    "\n",
    "with open(MODEL2FEATURES, 'wb') as handle:\n",
    "    pickle.dump(filename2featuresAll, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
